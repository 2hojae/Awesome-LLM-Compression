# Awesome-LLM-Compression [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
Awesome LLM compression research papers and tools. 

# Contents

- [Papers](#papers)
  - [Quantization](#general)
  - [Pruning](#architecture)
  - [Sparsity](#quantization)
  - [Distillation](#binarization)
- [Tools](#tools)

## Papers

### Quantization

- ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers <br> NeurIPS 2022 [[Paper]](https://arxiv.org/abs/2206.01861) [[Code]](https://github.com/microsoft/DeepSpeed)

- LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale <br> NeurIPS 2022 [[Paper]](https://arxiv.org/abs/2208.07339) [[Code]](https://github.com/TimDettmers/bitsandbytes)

- LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models <br> Arxiv 2022 [[Paper]](https://arxiv.org/abs/2206.09557) 

- Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling
Arxiv 2023 [[Paper]](https://arxiv.org/abs/2304.09145)

- Quantized Distributed Training of Large Models with Convergence Guarantees
Arxiv 2023 [[Paper]](https://arxiv.org/abs/2302.02390)

- SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models
ICML 2023 [[Paper]](https://arxiv.org/abs/2211.10438) [[Code]](https://github.com/mit-han-lab/smoothquant)

- GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers 
ICLR 2023 [[Paper]](https://arxiv.org/abs/2210.17323) [[Code]](https://github.com/IST-DASLab/gptq)

- QLoRA: Efficient Finetuning of Quantized LLMs
Arxiv 2023 [[Paper]](https://arxiv.org/abs/2305.14314) [[Code]](https://github.com/artidoro/qlora)

### Pruning

### Sparsity

### Pruning

### Distillation

## Tools
  


